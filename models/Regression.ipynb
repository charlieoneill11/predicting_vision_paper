{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "378fbfe2-9187-4c85-9d21-d7d3ec81c620",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"~/Documents/Github/paper/input/df_3_years.csv\")\n",
    "\n",
    "X, y = df.drop(columns=['target_va']), df.target_va.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "def score(model, X, y, cv=5, scoring='neg_root_mean_squared_error'):\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring=scoring)\n",
    "    return abs(np.mean(scores)), np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "feec9ea7-ce2a-4ba2-a0e6-a66ebed8f5bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68.95886889460154"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"~/Documents/Github/paper/input/df_3_years.csv\")\n",
    "np.mean(df.first_va)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "954beaa9-50b3-43b4-8df2-425d062bd705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69.2508858687054"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(df.mean_vision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e36eb4f-303b-42a8-aa48-582d537568f6",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "e57626bf-3854-41d5-824c-b26a03ef9866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.421478237960166"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds = np.array(X.first_va.values)\n",
    "np.sqrt(mean_squared_error(y_preds, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f663413-e43e-4b6e-9a03-5e7177a93f96",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3527b327-2011-4078-a889-0a758d334eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.192792859502166"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators = [('lr', LinearRegression()), ('gb', GradientBoostingRegressor(random_state=42))]\n",
    "reg = StackingRegressor(estimators=estimators,\n",
    "                        final_estimator=RandomForestRegressor(n_estimators=10, random_state=42))\n",
    "reg.fit(X_train, y_train)\n",
    "y_preds = reg.predict(X_test)\n",
    "np.sqrt(mean_squared_error(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aa037d8e-d511-45b4-baf6-a5b88edbfc53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7.8433518015477635, 0.2670546268521039)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(reg, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a201c691-709e-4201-9607-3976fccc9201",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "efc5b28c-bf07-4c8a-8d56-96e74f8007b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "61153532-381b-437f-8de5-5a6bbd4fdaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(14, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.out = nn.Linear(16, 1)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(14)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(32)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(torch.relu(self.fc1(self.batchnorm1(x))))\n",
    "        x = self.dropout(torch.relu(self.fc2(self.batchnorm2(x))))    \n",
    "        output = self.out(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "60521c23-887e-4019-95b9-41e31f63ab9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=5, kernel_size=4, padding=2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=5, out_channels=2, kernel_size=4, padding=2)\n",
    "        self.layer1 = nn.Sequential(self.conv1, nn.ReLU())\n",
    "        self.layer2 = nn.Sequential(self.conv2, nn.ReLU())         \n",
    "        # fully connected layer, output 2 classes\n",
    "        self.layer3 = nn.Sequential(nn.Linear(32, 64), nn.ReLU(),\n",
    "                                    nn.Linear(64, 16), nn.ReLU(),\n",
    "                                    nn.Linear(16, 8), nn.ReLU())\n",
    "        self.out = nn.Linear(8, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x.unsqueeze(1))\n",
    "        x = self.layer2(x)\n",
    "        # flatten the output of conv2 to (batch_size, 32)\n",
    "        x = x.view(x.size(0), -1) \n",
    "        x = self.layer3(x)\n",
    "        output = self.out(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "bd761c46-ea2c-4cee-b404-23d8bf089a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PytorchKfolds:\n",
    "    \n",
    "    def __init__(self, n_epochs=30):\n",
    "        self.df = pd.read_csv(\"~/Documents/Github/paper/input/df_2_years.csv\")\n",
    "        self.kdf = self.create_folds(self.df)\n",
    "        self.n_epochs = n_epochs\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "    \n",
    "    def create_folds(self, df):\n",
    "        # we create a new column called kfold and fill it with -1\n",
    "        df[\"kfold\"] = -1\n",
    "        # the next step is to randomize the rows of the data\n",
    "        df = df.sample(frac=1).reset_index(drop=True)\n",
    "        # fetch labels\n",
    "        y = df.target_va.values\n",
    "        # initiate the kfold class from model_selection module\n",
    "        kf = model_selection.KFold(n_splits=5)\n",
    "        # fill the new kfold column\n",
    "        for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):\n",
    "            df.loc[v_, 'kfold'] = f\n",
    "        return df\n",
    "    \n",
    "    def inputs_targets(self, df, fold):\n",
    "        df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "        df_train.fillna(df_train.mean(), inplace=True)\n",
    "        df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "        df_valid.fillna(df_valid.mean(), inplace=True)\n",
    "        X_train = df_train.drop(columns=[\"target_va\", \"kfold\"]).values\n",
    "        y_train = df_train.target_va.values\n",
    "        X_valid = df_valid.drop(columns=[\"target_va\", \"kfold\"]).values\n",
    "        y_valid = df_valid.target_va.values\n",
    "        return X_train, X_valid, y_train, y_valid\n",
    "    \n",
    "    def train_test_kfold(self, df, fold):\n",
    "        X_train, X_test, y_train, y_test = self.inputs_targets(df, fold)\n",
    "        # scale the data\n",
    "        ss = StandardScaler()\n",
    "        mm = MinMaxScaler()\n",
    "        X_train, X_test = ss.fit_transform(X_train), ss.fit_transform(X_test)\n",
    "        y_train = mm.fit_transform(y_train.reshape(-1, 1))\n",
    "        y_test = mm.fit_transform(y_test.reshape(-1, 1))\n",
    "        # convert to tensors\n",
    "        X_train_tensors = Variable(torch.Tensor(X_train))\n",
    "        X_test_tensors = Variable(torch.Tensor(X_test))\n",
    "        y_train_tensors = Variable(torch.Tensor(y_train))\n",
    "        y_test_tensors = Variable(torch.Tensor(y_test))\n",
    "        return X_train_tensors, X_test_tensors, y_train_tensors, y_test_tensors\n",
    "    \n",
    "    def create_dataloaders(self, X_train, X_test, y_train, y_test):\n",
    "        train_data, test_data = [], []\n",
    "        for i in range(len(X_train)):\n",
    "            train_data.append([X_train[i].to(torch.float32), \n",
    "                               y_train[i].type(torch.float32)])\n",
    "        for i in range(len(X_test)):\n",
    "            test_data.append([X_test[i].to(torch.float32), \n",
    "                               y_test[i].type(torch.float32)])\n",
    "        train_loader = torch.utils.data.DataLoader(train_data, shuffle=False, \n",
    "                                               batch_size=64, drop_last=True)\n",
    "        test_loader = torch.utils.data.DataLoader(test_data, shuffle=False, \n",
    "                                                  batch_size=len(X_test))\n",
    "        return train_loader, test_loader\n",
    "    \n",
    "    def reset_weights(self, m):\n",
    "        for layer in m.children():\n",
    "            if hasattr(layer, 'reset_parameters'):\n",
    "                layer.reset_parameters()\n",
    "        \n",
    "    def rmse_score(self, val_loader, model):\n",
    "        model.eval()\n",
    "        rmses = []\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in val_loader:\n",
    "                outputs = model(imgs).detach().numpy()\n",
    "                labels = labels.detach().numpy()\n",
    "                rmse = np.sqrt(mean_squared_error(outputs, labels))\n",
    "                rmses.append(rmse)\n",
    "            return np.round(np.mean(rmses), 4)\n",
    "                \n",
    "    \n",
    "    def training_loop(self, train_loader, val_loader, verbose=0):\n",
    "        n_epochs=self.n_epochs\n",
    "        model=Net()\n",
    "        model.apply(self.reset_weights)\n",
    "        optimiser=torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "        loss_fn=self.loss_fn\n",
    "        model.train()\n",
    "        count, best_rmse = 0, 1000\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            loss_train, loss_test = 0.0, 0.0\n",
    "            for imgs, labels in train_loader:\n",
    "                b_x = Variable(imgs)   # batch x\n",
    "                b_y = Variable(labels)   # batch y\n",
    "                outputs = model(imgs)\n",
    "                loss = loss_fn(outputs, b_y)\n",
    "                optimiser.zero_grad()\n",
    "                loss.backward()\n",
    "                optimiser.step()\n",
    "                loss_train += loss.item() # .item() is used to escape gradient\n",
    "            rmse = 100*self.rmse_score(val_loader, model)\n",
    "            # early stopping\n",
    "            if (best_rmse < rmse) and count > 40: \n",
    "                print(\"EARLY STOPPING INITIATED.\")\n",
    "                print(f\"Best RMSE = {best_rmse}. Epoch = {epoch}.\")\n",
    "                break\n",
    "            else: \n",
    "                if best_rmse > rmse: best_rmse = rmse\n",
    "                count += 1\n",
    "            if (epoch == 1 or epoch % (n_epochs/10) == 0) and verbose>1:\n",
    "                print(\"Epoch {}, Training Loss {}, RMSE {}%\".format(\n",
    "                    epoch,\n",
    "                    np.round(loss_train / len(train_loader), 4),\n",
    "                    np.round(rmse, 2)))\n",
    "        test_rmse = 100*self.rmse_score(val_loader, model)\n",
    "        if verbose==1: print(f\"Test RMSE = {test_rmse}\")\n",
    "        return test_rmse\n",
    "                \n",
    "    def kfold_train(self, verbose=0):\n",
    "        rmses = []\n",
    "        for i in range(5):\n",
    "            if verbose > 0: print(f'FOLD {i}')\n",
    "            X_train, X_test, y_train, y_test = self.train_test_kfold(self.kdf, i)\n",
    "            train_loader, val_loader = self.create_dataloaders(X_train, X_test, y_train, y_test)\n",
    "            test_rmse = self.training_loop(train_loader=train_loader,\n",
    "                                           val_loader=val_loader, verbose=verbose)\n",
    "            rmses.append(np.round(test_rmse, 2))\n",
    "            if verbose > 0: print('--------------------------------')\n",
    "        if verbose!=-1:\n",
    "            print(\"FINAL RESULTS\")\n",
    "            print(f\"Mean validation RMSE: {round(np.mean(rmses), 2)} (+/- {round(np.std(rmses), 2)})\")\n",
    "        if verbose==-1: return np.mean(rmses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "e697f458-9457-4d12-b2c5-0478c684310c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "EARLY STOPPING INITIATED.\n",
      "Best RMSE = 13.040000200271606. Epoch = 42.\n",
      "Test RMSE = 15.070000290870667\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "EARLY STOPPING INITIATED.\n",
      "Best RMSE = 10.80000028014183. Epoch = 42.\n",
      "Test RMSE = 11.209999769926071\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "EARLY STOPPING INITIATED.\n",
      "Best RMSE = 11.320000141859055. Epoch = 42.\n",
      "Test RMSE = 12.520000338554382\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "EARLY STOPPING INITIATED.\n",
      "Best RMSE = 16.86999946832657. Epoch = 42.\n",
      "Test RMSE = 17.139999568462372\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "EARLY STOPPING INITIATED.\n",
      "Best RMSE = 12.219999730587006. Epoch = 42.\n",
      "Test RMSE = 14.800000190734863\n",
      "--------------------------------\n",
      "FINAL RESULTS\n",
      "Mean validation RMSE: 14.15 (+/- 2.07)\n"
     ]
    }
   ],
   "source": [
    "ete = PytorchKfolds(n_epochs=2000)\n",
    "ete.kfold_train(verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "7b68d151-eedf-4f00-bc31-99552a25096b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_search(start, stop, step):\n",
    "    best_rmse, best_epoch = 0.0, 0\n",
    "    for i in range(start, stop, step):\n",
    "        ete = PytorchKfolds(n_epochs=i)\n",
    "        rmse = ete.kfold_train(verbose=-1)\n",
    "        if rmse > best_rmse:\n",
    "            best_rmse = rmse\n",
    "            best_epoch = i\n",
    "    return best_rmse, best_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3ce98b-da56-4f3d-9563-e10c7b83a7e7",
   "metadata": {},
   "source": [
    "## TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ce055169-b647-4e2f-b0e4-c5056f95858a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 495 with best_epoch = 395 and best_val_0_rmse = 10.91941\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    }
   ],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "import sklearn\n",
    "\n",
    "df = pd.read_csv(\"~/Documents/Github/paper/input/df_3_years.csv\")\n",
    "\n",
    "X, y = df.drop(columns=['target_va']).values, df.target_va.values.reshape(-1, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "tabnet_params = {\"optimizer_fn\":torch.optim.Adam,\n",
    "                 \"verbose\":0,\n",
    "                 \"optimizer_params\":dict(lr=2e-2),\n",
    "                 \"scheduler_params\":{\"step_size\":50, # how to use learning rate scheduler\n",
    "                                 \"gamma\":0.9},\n",
    "                 \"scheduler_fn\":torch.optim.lr_scheduler.StepLR,\n",
    "                 \"mask_type\":'entmax' # \"sparsemax\"\n",
    "                }\n",
    "\n",
    "clf = TabNetRegressor(**tabnet_params) \n",
    "clf.fit(X_train, y_train, eval_set=[(X_test, y_test)],\n",
    "        eval_metric=['rmse'], patience=100,\n",
    "        max_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e942e2d3-bee0-43b7-a303-ae53989a43cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.919407111237296"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = clf.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef7f68b-9897-4d3e-ae49-2085712a0482",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
